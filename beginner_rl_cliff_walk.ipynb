{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX9WjeHwsmrZ"
      },
      "source": [
        "# Cliff walking intro\n",
        "\n",
        "Just a quick intro to [cliff walking](https://www.gymlibrary.dev/environments/toy_text/cliff_walking/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NzqfFc2twjs"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44DaeN_MnyD5"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.6 64-bit' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '\"c:/Program Files/Python310/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-s1zkjE_pKJa"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import pandas\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdC6H6sHtga0"
      },
      "source": [
        "# First steps\n",
        "\n",
        "Here we initialise the environment, take a few random steps, and print the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx5girT3piDF"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CliffWalking-v0\", render_mode=\"ansi\", new_step_api=True)\n",
        "env.reset()\n",
        "\n",
        "def draw(env):\n",
        "  for line in env.render()[0].split('\\n'):\n",
        "    print(line)\n",
        "\n",
        "def get_random_action(env):\n",
        "  return env.action_space.sample()\n",
        "\n",
        "print(\"initial state:\")\n",
        "draw(env)\n",
        "\n",
        "for _ in range(3):\n",
        "  action = get_random_action(env)\n",
        "  env.step(action)\n",
        "  print(f\"state after action {action}:\")\n",
        "  draw(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl0fUd86xCsE"
      },
      "source": [
        "# Collect training data\n",
        "\n",
        "Random agent, lots of games (episodes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KCJkqm8xJW-"
      },
      "outputs": [],
      "source": [
        "num_episodes = 100\n",
        "episodes = []\n",
        "episode_length_limit = 100\n",
        "\n",
        "for i in range(num_episodes):\n",
        "    old_observation = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    steps = []\n",
        "    while not done and len(steps) < episode_length_limit:\n",
        "        new_action = get_random_action(env)\n",
        "        observation, reward, done, trunc, info = env.step(new_action)\n",
        "        total_reward += reward\n",
        "        \n",
        "        steps.append({\n",
        "            \"observation\": old_observation,\n",
        "            \"action\": new_action,\n",
        "            \"reward\": reward,\n",
        "            \"episode\": i,\n",
        "        })\n",
        "        old_observation = observation\n",
        "        \n",
        "    # incorporate total reward\n",
        "    num_steps = len(steps)\n",
        "    for i, step in enumerate(steps):\n",
        "        step[\"tot_reward\"] = total_reward\n",
        "        step[\"decay_reward\"] = i * total_reward / num_steps\n",
        "        \n",
        "    episodes.extend(steps)\n",
        "    \n",
        "episodes_df = pandas.DataFrame(episodes)\n",
        "episodes_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8FGlG-T1Zi5"
      },
      "source": [
        "# todo\n",
        "- train a network. just a simple one - figure out how to get the right input & output shape\n",
        "- use the network. better results than random?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
